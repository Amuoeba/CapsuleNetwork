{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tutorial\n",
    "## Basics\n",
    "Tensors in pytorch. x = num_rows, y=num_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision        \n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23erik\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3134a10d31af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "a = str(23)\n",
    "l = [[1,2,3,4,5],[\"a\",\"b\"],[5,6,7]]\n",
    "b = \"erik\"\n",
    "c = a + b\n",
    "print(c)\n",
    "print(l[:1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenslist sizes: torch.Size([5, 3, 3])\n",
      "tenslist: [tensor([[[ 30,  38,  13],\n",
      "         [ 73,  10,  62],\n",
      "         [ 29,  47,  48]],\n",
      "\n",
      "        [[ 57,  71,  99],\n",
      "         [ 33,  37,  87],\n",
      "         [ 26,   1,  66]],\n",
      "\n",
      "        [[ 23,  15,  84],\n",
      "         [ 92,  87,  98],\n",
      "         [ 17,  31,   3]],\n",
      "\n",
      "        [[ 48,  98,   1],\n",
      "         [ 83,  61,  89],\n",
      "         [ 70,   9,  44]],\n",
      "\n",
      "        [[  9,  27,  18],\n",
      "         [ 67,  45,  74],\n",
      "         [ 42,   7,   2]]]), tensor([[[ 30,  38,  13],\n",
      "         [ 73,  10,  62],\n",
      "         [ 29,  47,  48]],\n",
      "\n",
      "        [[ 57,  71,  99],\n",
      "         [ 33,  37,  87],\n",
      "         [ 26,   1,  66]],\n",
      "\n",
      "        [[ 23,  15,  84],\n",
      "         [ 92,  87,  98],\n",
      "         [ 17,  31,   3]],\n",
      "\n",
      "        [[ 48,  98,   1],\n",
      "         [ 83,  61,  89],\n",
      "         [ 70,   9,  44]],\n",
      "\n",
      "        [[  9,  27,  18],\n",
      "         [ 67,  45,  74],\n",
      "         [ 42,   7,   2]]])]\n",
      "Stacked 1 Size: torch.Size([5, 3, 3, 2])\n",
      "Stacked: tensor([[[[ 30,  30],\n",
      "          [ 38,  38],\n",
      "          [ 13,  13]],\n",
      "\n",
      "         [[ 73,  73],\n",
      "          [ 10,  10],\n",
      "          [ 62,  62]],\n",
      "\n",
      "         [[ 29,  29],\n",
      "          [ 47,  47],\n",
      "          [ 48,  48]]],\n",
      "\n",
      "\n",
      "        [[[ 57,  57],\n",
      "          [ 71,  71],\n",
      "          [ 99,  99]],\n",
      "\n",
      "         [[ 33,  33],\n",
      "          [ 37,  37],\n",
      "          [ 87,  87]],\n",
      "\n",
      "         [[ 26,  26],\n",
      "          [  1,   1],\n",
      "          [ 66,  66]]],\n",
      "\n",
      "\n",
      "        [[[ 23,  23],\n",
      "          [ 15,  15],\n",
      "          [ 84,  84]],\n",
      "\n",
      "         [[ 92,  92],\n",
      "          [ 87,  87],\n",
      "          [ 98,  98]],\n",
      "\n",
      "         [[ 17,  17],\n",
      "          [ 31,  31],\n",
      "          [  3,   3]]],\n",
      "\n",
      "\n",
      "        [[[ 48,  48],\n",
      "          [ 98,  98],\n",
      "          [  1,   1]],\n",
      "\n",
      "         [[ 83,  83],\n",
      "          [ 61,  61],\n",
      "          [ 89,  89]],\n",
      "\n",
      "         [[ 70,  70],\n",
      "          [  9,   9],\n",
      "          [ 44,  44]]],\n",
      "\n",
      "\n",
      "        [[[  9,   9],\n",
      "          [ 27,  27],\n",
      "          [ 18,  18]],\n",
      "\n",
      "         [[ 67,  67],\n",
      "          [ 45,  45],\n",
      "          [ 74,  74]],\n",
      "\n",
      "         [[ 42,  42],\n",
      "          [  7,   7],\n",
      "          [  2,   2]]]])\n",
      "torch.Size([1, 2, 5, 3, 3])\n",
      "tensor([[[[[ 73,  10,  62],\n",
      "           [ 29,  47,  48],\n",
      "           [ 73,   4,  54]],\n",
      "\n",
      "          [[ 33,  37,  87],\n",
      "           [ 26,   1,  66],\n",
      "           [ 66,  53,  38]],\n",
      "\n",
      "          [[ 92,  87,  98],\n",
      "           [ 17,  31,   3],\n",
      "           [ 91,  49,  74]],\n",
      "\n",
      "          [[ 83,  61,  89],\n",
      "           [ 70,   9,  44],\n",
      "           [  1,  60,  14]],\n",
      "\n",
      "          [[ 67,  45,  74],\n",
      "           [ 42,   7,   2],\n",
      "           [ 81,  53,  54]]],\n",
      "\n",
      "\n",
      "         [[[ 81,  14,   3],\n",
      "           [ 52,  66,  69],\n",
      "           [  9,  51,   5]],\n",
      "\n",
      "          [[  8,  44,  52],\n",
      "           [ 31,  64,  70],\n",
      "           [ 78,  27,  58]],\n",
      "\n",
      "          [[ 71,   1,  60],\n",
      "           [ 74,  25,  26],\n",
      "           [ 39,  36,  73]],\n",
      "\n",
      "          [[ 56,  37,  32],\n",
      "           [ 53,  36,  65],\n",
      "           [  9,  14,  98]],\n",
      "\n",
      "          [[ 15,  37,  89],\n",
      "           [  9,  76,  94],\n",
      "           [ 78,  65,  14]]]]])\n",
      "Split: [tensor([[[[[ 73,  10,  62],\n",
      "           [ 29,  47,  48],\n",
      "           [ 73,   4,  54]],\n",
      "\n",
      "          [[ 33,  37,  87],\n",
      "           [ 26,   1,  66],\n",
      "           [ 66,  53,  38]],\n",
      "\n",
      "          [[ 92,  87,  98],\n",
      "           [ 17,  31,   3],\n",
      "           [ 91,  49,  74]],\n",
      "\n",
      "          [[ 83,  61,  89],\n",
      "           [ 70,   9,  44],\n",
      "           [  1,  60,  14]],\n",
      "\n",
      "          [[ 67,  45,  74],\n",
      "           [ 42,   7,   2],\n",
      "           [ 81,  53,  54]]]]]), tensor([[[[[ 81,  14,   3],\n",
      "           [ 52,  66,  69],\n",
      "           [  9,  51,   5]],\n",
      "\n",
      "          [[  8,  44,  52],\n",
      "           [ 31,  64,  70],\n",
      "           [ 78,  27,  58]],\n",
      "\n",
      "          [[ 71,   1,  60],\n",
      "           [ 74,  25,  26],\n",
      "           [ 39,  36,  73]],\n",
      "\n",
      "          [[ 56,  37,  32],\n",
      "           [ 53,  36,  65],\n",
      "           [  9,  14,  98]],\n",
      "\n",
      "          [[ 15,  37,  89],\n",
      "           [  9,  76,  94],\n",
      "           [ 78,  65,  14]]]]])]\n",
      "Stacked: tensor([[[[ 73,  81],\n",
      "          [ 10,  14],\n",
      "          [ 62,   3]],\n",
      "\n",
      "         [[ 29,  52],\n",
      "          [ 47,  66],\n",
      "          [ 48,  69]],\n",
      "\n",
      "         [[ 73,   9],\n",
      "          [  4,  51],\n",
      "          [ 54,   5]]],\n",
      "\n",
      "\n",
      "        [[[ 33,   8],\n",
      "          [ 37,  44],\n",
      "          [ 87,  52]],\n",
      "\n",
      "         [[ 26,  31],\n",
      "          [  1,  64],\n",
      "          [ 66,  70]],\n",
      "\n",
      "         [[ 66,  78],\n",
      "          [ 53,  27],\n",
      "          [ 38,  58]]],\n",
      "\n",
      "\n",
      "        [[[ 92,  71],\n",
      "          [ 87,   1],\n",
      "          [ 98,  60]],\n",
      "\n",
      "         [[ 17,  74],\n",
      "          [ 31,  25],\n",
      "          [  3,  26]],\n",
      "\n",
      "         [[ 91,  39],\n",
      "          [ 49,  36],\n",
      "          [ 74,  73]]],\n",
      "\n",
      "\n",
      "        [[[ 83,  56],\n",
      "          [ 61,  37],\n",
      "          [ 89,  32]],\n",
      "\n",
      "         [[ 70,  53],\n",
      "          [  9,  36],\n",
      "          [ 44,  65]],\n",
      "\n",
      "         [[  1,   9],\n",
      "          [ 60,  14],\n",
      "          [ 14,  98]]],\n",
      "\n",
      "\n",
      "        [[[ 67,  15],\n",
      "          [ 45,  37],\n",
      "          [ 74,  89]],\n",
      "\n",
      "         [[ 42,   9],\n",
      "          [  7,  76],\n",
      "          [  2,  94]],\n",
      "\n",
      "         [[ 81,  78],\n",
      "          [ 53,  65],\n",
      "          [ 54,  14]]]])\n",
      "Stacked size: torch.Size([5, 3, 3, 2])\n",
      "ex_tens view: tensor([[[ 73,  10],\n",
      "         [ 62,  29],\n",
      "         [ 47,  48],\n",
      "         [ 73,   4],\n",
      "         [ 54,  33],\n",
      "         [ 37,  87],\n",
      "         [ 26,   1],\n",
      "         [ 66,  66],\n",
      "         [ 53,  38],\n",
      "         [ 92,  87],\n",
      "         [ 98,  17],\n",
      "         [ 31,   3],\n",
      "         [ 91,  49],\n",
      "         [ 74,  83],\n",
      "         [ 61,  89],\n",
      "         [ 70,   9],\n",
      "         [ 44,   1],\n",
      "         [ 60,  14],\n",
      "         [ 67,  45],\n",
      "         [ 74,  42],\n",
      "         [  7,   2],\n",
      "         [ 81,  53],\n",
      "         [ 54,  81],\n",
      "         [ 14,   3],\n",
      "         [ 52,  66],\n",
      "         [ 69,   9],\n",
      "         [ 51,   5],\n",
      "         [  8,  44],\n",
      "         [ 52,  31],\n",
      "         [ 64,  70],\n",
      "         [ 78,  27],\n",
      "         [ 58,  71],\n",
      "         [  1,  60],\n",
      "         [ 74,  25],\n",
      "         [ 26,  39],\n",
      "         [ 36,  73],\n",
      "         [ 56,  37],\n",
      "         [ 32,  53],\n",
      "         [ 36,  65],\n",
      "         [  9,  14],\n",
      "         [ 98,  15],\n",
      "         [ 37,  89],\n",
      "         [  9,  76],\n",
      "         [ 94,  78],\n",
      "         [ 65,  14]]])\n",
      "stacked view: tensor([[[ 73,  81],\n",
      "         [ 10,  14],\n",
      "         [ 62,   3],\n",
      "         [ 29,  52],\n",
      "         [ 47,  66],\n",
      "         [ 48,  69],\n",
      "         [ 73,   9],\n",
      "         [  4,  51],\n",
      "         [ 54,   5],\n",
      "         [ 33,   8],\n",
      "         [ 37,  44],\n",
      "         [ 87,  52],\n",
      "         [ 26,  31],\n",
      "         [  1,  64],\n",
      "         [ 66,  70],\n",
      "         [ 66,  78],\n",
      "         [ 53,  27],\n",
      "         [ 38,  58],\n",
      "         [ 92,  71],\n",
      "         [ 87,   1],\n",
      "         [ 98,  60],\n",
      "         [ 17,  74],\n",
      "         [ 31,  25],\n",
      "         [  3,  26],\n",
      "         [ 91,  39],\n",
      "         [ 49,  36],\n",
      "         [ 74,  73],\n",
      "         [ 83,  56],\n",
      "         [ 61,  37],\n",
      "         [ 89,  32],\n",
      "         [ 70,  53],\n",
      "         [  9,  36],\n",
      "         [ 44,  65],\n",
      "         [  1,   9],\n",
      "         [ 60,  14],\n",
      "         [ 14,  98],\n",
      "         [ 67,  15],\n",
      "         [ 45,  37],\n",
      "         [ 74,  89],\n",
      "         [ 42,   9],\n",
      "         [  7,  76],\n",
      "         [  2,  94],\n",
      "         [ 81,  78],\n",
      "         [ 53,  65],\n",
      "         [ 54,  14]]])\n",
      "Stacked 1 view: tensor([[[ 30,  30],\n",
      "         [ 38,  38],\n",
      "         [ 13,  13],\n",
      "         [ 73,  73],\n",
      "         [ 10,  10],\n",
      "         [ 62,  62],\n",
      "         [ 29,  29],\n",
      "         [ 47,  47],\n",
      "         [ 48,  48],\n",
      "         [ 57,  57],\n",
      "         [ 71,  71],\n",
      "         [ 99,  99],\n",
      "         [ 33,  33],\n",
      "         [ 37,  37],\n",
      "         [ 87,  87],\n",
      "         [ 26,  26],\n",
      "         [  1,   1],\n",
      "         [ 66,  66],\n",
      "         [ 23,  23],\n",
      "         [ 15,  15],\n",
      "         [ 84,  84],\n",
      "         [ 92,  92],\n",
      "         [ 87,  87],\n",
      "         [ 98,  98],\n",
      "         [ 17,  17],\n",
      "         [ 31,  31],\n",
      "         [  3,   3],\n",
      "         [ 48,  48],\n",
      "         [ 98,  98],\n",
      "         [  1,   1],\n",
      "         [ 83,  83],\n",
      "         [ 61,  61],\n",
      "         [ 89,  89],\n",
      "         [ 70,  70],\n",
      "         [  9,   9],\n",
      "         [ 44,  44],\n",
      "         [  9,   9],\n",
      "         [ 27,  27],\n",
      "         [ 18,  18],\n",
      "         [ 67,  67],\n",
      "         [ 45,  45],\n",
      "         [ 74,  74],\n",
      "         [ 42,  42],\n",
      "         [  7,   7],\n",
      "         [  2,   2]]])\n"
     ]
    }
   ],
   "source": [
    "def util_random_tens(tup):   \n",
    "    class Inner(): \n",
    "        no = 2        \n",
    "        def __init__():\n",
    "            pass           \n",
    "            \n",
    "        @staticmethod\n",
    "        def aux_inner(tup,no_call=5):\n",
    "            \n",
    "            if len(tup) == 1:\n",
    "                Inner.no += 1\n",
    "                s = no_call            \n",
    "                random.seed(s)\n",
    "                rand_list = random.sample(range(0,100),tup[0])            \n",
    "                return rand_list        \n",
    "            else:\n",
    "                l = []\n",
    "                for i in range(tup[0]):\n",
    "                    Inner.no += 1\n",
    "                    no_call += 1\n",
    "                    l.append(Inner.aux_inner(tup[1:],no_call=Inner.no))\n",
    "                return l\n",
    "            \n",
    "    \n",
    "    return Inner.aux_inner(tup)\n",
    "        \n",
    "\n",
    "ex_tens = torch.tensor(util_random_tens((1,2,5,3,3)))\n",
    "tensList = [torch.tensor(util_random_tens((5,3,3)))] * 2\n",
    "print(\"Tenslist sizes:\",tensList[0].size())\n",
    "print(\"tenslist:\",tensList)\n",
    "stacked1 = torch.stack(tensList,3)\n",
    "print(\"Stacked 1 Size:\",stacked1.size())\n",
    "print(\"Stacked:\",stacked1)\n",
    "print(ex_tens.size())\n",
    "print(ex_tens)\n",
    "split = list(torch.split(ex_tens,int(ex_tens.size(1)/ex_tens.size(1)),dim=1))\n",
    "print(\"Split:\",split)\n",
    "stacked = torch.stack(split,5).squeeze()name\n",
    "print(\"Stacked:\",stacked)\n",
    "\n",
    "print(\"Stacked size:\",stacked.size())\n",
    "print(\"ex_tens view:\",ex_tens.view(1,45,-1))\n",
    "print(\"stacked view:\",stacked.view(1,45,-1))\n",
    "print(\"Stacked 1 view:\",stacked1.view(1,45,-1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.9795,  0.0437],\n",
      "          [ 0.6934,  0.3181],\n",
      "          [ 0.1216,  0.8136]],\n",
      "\n",
      "         [[ 0.6256,  0.9458],\n",
      "          [ 0.6875,  0.5891],\n",
      "          [ 0.2683,  0.5889]],\n",
      "\n",
      "         [[ 0.4347,  0.4364],\n",
      "          [ 0.6285,  0.8506],\n",
      "          [ 0.6217,  0.1045]],\n",
      "\n",
      "         [[ 0.4277,  0.5513],\n",
      "          [ 0.1635,  0.0882],\n",
      "          [ 0.1174,  0.1637]],\n",
      "\n",
      "         [[ 0.1236,  0.2208],\n",
      "          [ 0.2796,  0.9796],\n",
      "          [ 0.3950,  0.8690]]]])\n",
      "Size: torch.Size([1, 5, 3, 2])\n",
      "tensor([[[ 0.9795,  0.0437],\n",
      "         [ 0.6256,  0.9458]]])\n"
     ]
    }
   ],
   "source": [
    "sh = torch.rand(1,5,3,2)\n",
    "print(sh)\n",
    "print(\"Size:\", sh.size())\n",
    "print(sh[:,:2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4439,  0.4826,  0.7466],\n",
      "        [ 0.5812,  0.9706,  0.9643],\n",
      "        [ 0.3979,  0.4532,  0.1923],\n",
      "        [ 0.2714,  0.1722,  0.5221],\n",
      "        [ 0.7439,  0.9141,  0.5705]])\n",
      "tensor([[ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0]])\n",
      "tensor(0.9706)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "zeros = torch.zeros(3,5,dtype = torch.long)\n",
    "print(x)\n",
    "print(zeros)\n",
    "print(torch.max(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing tensors directly from data and priting its size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6]])\n",
      "True\n",
      "tensor([[ 8,  7,  6],\n",
      "        [ 5,  4,  3]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2,3],[4,5,6]]\n",
    "data_t = torch.tensor(data)\n",
    "print(data_t)\n",
    "print(data_t.size() == torch.Size([2,3]))\n",
    "print(9-data_t)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.]])\n",
      "tensor([[[ 1.],\n",
      "         [ 0.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 0.],\n",
      "         [ 1.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 0.],\n",
      "         [ 0.],\n",
      "         [ 1.]]])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "eye = torch.eye(3)\n",
    "print(eye)\n",
    "print(eye[:,:,None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resutl of matrix multiply 1: \n",
      " tensor([[ 14.,  13.]], dtype=torch.float64)\n",
      "Normal multiply:\n",
      "\n",
      "tensor([[ 2.,  6.],\n",
      "        [ 8.,  9.]], dtype=torch.float64)\n",
      "Square root and sum: \n",
      "\n",
      "tensor([ 5.,  5.], dtype=torch.float64)\n",
      "Two different transposes\n",
      "Original dim: torch.Size([1, 2, 3])\n",
      "tensor([[[  5,   6,   7]],\n",
      "\n",
      "        [[ 20,  21,  22]]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([[[  5],\n",
      "         [ 20]],\n",
      "\n",
      "        [[  6],\n",
      "         [ 21]],\n",
      "\n",
      "        [[  7],\n",
      "         [ 22]]])\n",
      "torch.Size([3, 2, 1])\n",
      "trans1.t()\n",
      "tensor([[  1,  10],\n",
      "        [  2,  11],\n",
      "        [  3,  13]])\n",
      "\n",
      "\n",
      "\n",
      "Matrix multiply:\n",
      "tensor([[  38,  128],\n",
      "        [ 207,  717]])\n",
      "Addition normal\n",
      "tensor([[  6,   8,  10],\n",
      "        [ 30,  32,  35]])\n",
      "Addition with pytorch function\n",
      "tensor([[  6,   8,  10],\n",
      "        [ 30,  32,  35]])\n",
      "Addition with providing output tensor\n",
      "tensor([[  6,   8,  10],\n",
      "        [ 30,  32,  35]])\n",
      "Addition in place (x is changed in this example)\n",
      "tensor([[  6,   8,  10],\n",
      "        [ 30,  32,  35]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[10,11,13]])\n",
    "y = torch.tensor([[5,6,7],[20,21,22]])\n",
    "trans = torch.tensor([[[5,6,7],[20,21,22]]])\n",
    "trans1 = torch.tensor([[1,2,3],[10,11,13]])\n",
    "mm1 = torch.tensor([[1,2],[4,3]]).double()\n",
    "mm2 = torch.tensor([[2,3]]).double()\n",
    "mmmm = torch.matmul(mm2,mm1)\n",
    "print(\"Resutl of matrix multiply 1: \\n\",mmmm)\n",
    "\n",
    "\n",
    "print(\"Normal multiply:\\n\")\n",
    "print(mm1*mm2)\n",
    "\n",
    "print(\"Square root and sum: \\n\")\n",
    "z = torch.sqrt(mm1**2).sum(-2)\n",
    "print(z)\n",
    "\n",
    "print(\"Two different transposes\")\n",
    "print(\"Original dim: {}\".format(trans.size()))\n",
    "print(trans.transpose(0,1))\n",
    "print(trans.transpose(0,1).size())\n",
    "print(trans.transpose(2,0))\n",
    "print(trans.transpose(2,0).size())\n",
    "print(\"trans1.t()\")\n",
    "print(trans1.t())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Matrix multiply:\")\n",
    "print(x.mm(y.transpose(0,1)))\n",
    "\n",
    "print(\"Addition normal\")\n",
    "print(x+y)\n",
    "\n",
    "print(\"Addition with pytorch function\")\n",
    "print(torch.add(x,y))\n",
    "\n",
    "print(\"Addition with providing output tensor\")\n",
    "result = torch.zeros(2,3, dtype = torch.long)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)\n",
    "\n",
    "print(\"Addition in place (x is changed in this example)\")\n",
    "x.add_(y)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ Operations that mutate a tensor in place are post ficed with \"_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing elements in tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2587,  0.6451,  0.4262,  0.4482],\n",
      "        [ 0.4279,  0.7340,  0.0909,  0.3557],\n",
      "        [ 0.8988,  0.3604,  0.7420,  0.6601],\n",
      "        [ 0.9830,  0.1302,  0.7470,  0.7972],\n",
      "        [ 0.3061,  0.2687,  0.5608,  0.5939],\n",
      "        [ 0.4654,  0.0722,  0.2099,  0.4110],\n",
      "        [ 0.1090,  0.5451,  0.1950,  0.5889]])\n",
      "Column 0\n",
      "tensor([ 0.2587,  0.4279,  0.8988,  0.9830,  0.3061,  0.4654,  0.1090])\n",
      "Row 0\n",
      "tensor([ 0.2587,  0.6451,  0.4262,  0.4482])\n",
      "Second and third row and second and third column\n",
      "tensor([[ 0.7340,  0.0909],\n",
      "        [ 0.3604,  0.7420]])\n"
     ]
    }
   ],
   "source": [
    "rn = torch.rand(7,4)\n",
    "print(rn)\n",
    "print(\"Column 0\")\n",
    "print(rn[:,0])\n",
    "print(\"Row 0\")\n",
    "print(rn[0,:])\n",
    "print(\"Second and third row and second and third column\")\n",
    "print(rn[1:3,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) \n",
      " tensor([[-0.5736,  0.8739,  1.1581, -0.4651],\n",
      "        [-1.2898, -2.0091,  0.0448, -0.7776],\n",
      "        [ 0.2598,  0.6121,  0.4177,  0.5769],\n",
      "        [ 0.5912,  0.5000,  2.4781,  0.4546]])\n",
      "torch.Size([16]) \n",
      " tensor([-0.5736,  0.8739,  1.1581, -0.4651, -1.2898, -2.0091,  0.0448,\n",
      "        -0.7776,  0.2598,  0.6121,  0.4177,  0.5769,  0.5912,  0.5000,\n",
      "         2.4781,  0.4546])\n",
      "torch.Size([2, 8]) \n",
      " tensor([[-0.5736,  0.8739,  1.1581, -0.4651, -1.2898, -2.0091,  0.0448,\n",
      "         -0.7776],\n",
      "        [ 0.2598,  0.6121,  0.4177,  0.5769,  0.5912,  0.5000,  2.4781,\n",
      "          0.4546]])\n",
      "torch.Size([1, 16]) \n",
      " tensor([[-0.5736,  0.8739,  1.1581, -0.4651, -1.2898, -2.0091,  0.0448,\n",
      "         -0.7776,  0.2598,  0.6121,  0.4177,  0.5769,  0.5912,  0.5000,\n",
      "          2.4781,  0.4546]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "v = x.view(1,-1)\n",
    "print(x.size(),\"\\n\",x)\n",
    "print(y.size(),\"\\n\",y)\n",
    "print(z.size(),\"\\n\",z)\n",
    "print(v.size(),\"\\n\",v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Tensors ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2574,  0.3589,  0.2550,  0.0728],\n",
      "         [ 0.6585,  0.1817,  0.2356,  0.6970],\n",
      "         [ 0.7150,  0.0318,  0.9021,  0.9439]],\n",
      "\n",
      "        [[ 0.6977,  0.5638,  0.4860,  0.2753],\n",
      "         [ 0.2124,  0.6545,  0.7921,  0.1981],\n",
      "         [ 0.0606,  0.8532,  0.7955,  0.3575]]])\n",
      "tensor([[[[[ 0.2574],\n",
      "           [ 0.3589],\n",
      "           [ 0.2550],\n",
      "           [ 0.0728]],\n",
      "\n",
      "          [[ 0.2574],\n",
      "           [ 0.3589],\n",
      "           [ 0.2550],\n",
      "           [ 0.0728]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6585],\n",
      "           [ 0.1817],\n",
      "           [ 0.2356],\n",
      "           [ 0.6970]],\n",
      "\n",
      "          [[ 0.6585],\n",
      "           [ 0.1817],\n",
      "           [ 0.2356],\n",
      "           [ 0.6970]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7150],\n",
      "           [ 0.0318],\n",
      "           [ 0.9021],\n",
      "           [ 0.9439]],\n",
      "\n",
      "          [[ 0.7150],\n",
      "           [ 0.0318],\n",
      "           [ 0.9021],\n",
      "           [ 0.9439]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.6977],\n",
      "           [ 0.5638],\n",
      "           [ 0.4860],\n",
      "           [ 0.2753]],\n",
      "\n",
      "          [[ 0.6977],\n",
      "           [ 0.5638],\n",
      "           [ 0.4860],\n",
      "           [ 0.2753]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2124],\n",
      "           [ 0.6545],\n",
      "           [ 0.7921],\n",
      "           [ 0.1981]],\n",
      "\n",
      "          [[ 0.2124],\n",
      "           [ 0.6545],\n",
      "           [ 0.7921],\n",
      "           [ 0.1981]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0606],\n",
      "           [ 0.8532],\n",
      "           [ 0.7955],\n",
      "           [ 0.3575]],\n",
      "\n",
      "          [[ 0.0606],\n",
      "           [ 0.8532],\n",
      "           [ 0.7955],\n",
      "           [ 0.3575]]]]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.rand(2,3,4)\n",
    "print(d)\n",
    "stacked = torch.stack([d] * 2,dim=2).unsqueeze(4)\n",
    "print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1:\n",
      "tensor([[[[  1,   2],\n",
      "          [  3,   4]]],\n",
      "\n",
      "\n",
      "        [[[  5,   6],\n",
      "          [  7,   8]]],\n",
      "\n",
      "\n",
      "        [[[  9,  10],\n",
      "          [ 11,  12]]]])\n",
      "Tensor 2:\n",
      "tensor([[[[ 11,  22],\n",
      "          [ 33,  44]]],\n",
      "\n",
      "\n",
      "        [[[  1,   2],\n",
      "          [  3,   4]]],\n",
      "\n",
      "\n",
      "        [[[  1,   2],\n",
      "          [  3,   4]]]])\n",
      "------------------\n",
      "Stacked tensors:\n",
      "tensor([[[[[  1,   2],\n",
      "           [  3,   4]]],\n",
      "\n",
      "\n",
      "         [[[ 11,  22],\n",
      "           [ 33,  44]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[  5,   6],\n",
      "           [  7,   8]]],\n",
      "\n",
      "\n",
      "         [[[  1,   2],\n",
      "           [  3,   4]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[  9,  10],\n",
      "           [ 11,  12]]],\n",
      "\n",
      "\n",
      "         [[[  1,   2],\n",
      "           [  3,   4]]]]])\n",
      "Tensor sizes: torch.Size([3, 1, 2, 2])\n",
      "Stacked sizes: torch.Size([3, 2, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "stack1 = tens = torch.tensor([[[[1,2],[3,4]]],[[[5,6],[7,8]]],[[[9,10],[11,12]]]])\n",
    "stack2 = tens = torch.tensor([[[[11,22],[33,44]]],[[[1,2],[3,4]]],[[[1,2],[3,4]]]])\n",
    "ls = [stack1, stack2]\n",
    "print(\"Tensor 1:\")\n",
    "print(stack1)\n",
    "print(\"Tensor 2:\")\n",
    "print(stack2)\n",
    "print(\"------------------\")\n",
    "print(\"Stacked tensors:\")\n",
    "stacked = torch.stack(ls,1) \n",
    "print(stacked)\n",
    "print(\"Tensor sizes:\",stack1.size())\n",
    "print(\"Stacked sizes:\",stacked.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Soft Max understanding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.],\n",
      "         [ 0.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 0.],\n",
      "         [ 0.],\n",
      "         [ 0.]]])\n",
      "tensor([[[ 0.1192],\n",
      "         [ 0.1192]],\n",
      "\n",
      "        [[ 0.8808],\n",
      "         [ 0.8808]]])\n",
      "tensor([ 0.0321,  0.0871,  0.2369,  0.6439])\n"
     ]
    }
   ],
   "source": [
    "te1 = torch.zeros([2,3,1])\n",
    "te2 = torch.zeros([2,2])\n",
    "exmpl1 = torch.tensor([[[1],[2]],[[3],[4]]],dtype=torch.float)\n",
    "exml2 = torch.tensor([1,2,3,4], dtype=torch.float)\n",
    "print(te1)\n",
    "softmax = F.softmax(exmpl1,dim = 0)\n",
    "sm2 = F.softmax(exml2,dim=0)\n",
    "print(softmax)\n",
    "print(sm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squashing function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "tensor([[[  5.,   4.,   9.]],\n",
      "\n",
      "        [[ 50.,  40.,  90.]]], dtype=torch.float64)\n",
      "tensor([[[ 2.2361,  2.0000,  3.0000]],\n",
      "\n",
      "        [[ 7.0711,  6.3246,  9.4868]]], dtype=torch.float64)\n",
      "tensor([[[ 0.4105,  0.8889,  0.9643],\n",
      "         [ 0.8210,  0.4444,  0.9643],\n",
      "         [ 0.8210,  0.4444,  0.9643]],\n",
      "\n",
      "        [[ 1.4102,  3.1498,  3.1586],\n",
      "         [ 2.8204,  1.5749,  3.1586],\n",
      "         [ 2.8204,  1.5749,  3.1586]]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-269ebbe3c89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCapsSquareNorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCapsSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCapsSquareNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mCapsOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCapsSquareNorm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mCapsSquareNorm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mCapsSum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCapsSquareNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "tens = torch.tensor([[[1,2,3],[2,1,3],[2,1,3]],[[10,20,30],[20,10,30],[20,10,30]]]).double()\n",
    "print(tens.size())\n",
    "CapsSquareNorm = tens.sum(1,keepdim=True)\n",
    "CapsSum = torch.sqrt(CapsSquareNorm)\n",
    "CapsOut = CapsSquareNorm*tens/(1+CapsSquareNorm*CapsSum)\n",
    "\n",
    "print(CapsSquareNorm)\n",
    "print(CapsSum)\n",
    "print(CapsOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplications (in batch learning) ##\n",
    "Input data has dimensions `[batch,in,hidden,out]`. Lets say is 1 hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1063,  0.1369],\n",
      "        [ 0.7666,  0.7842],\n",
      "        [ 0.4955,  0.6879]], dtype=torch.float64)\n",
      "tensor([[ 2.,  2.,  2.],\n",
      "        [ 5.,  5.,  5.]], dtype=torch.float64)\n",
      "tensor([[  2.,   2.,   2.,   2.,   2.],\n",
      "        [  5.,   5.,   5.,   5.,   5.],\n",
      "        [ 10.,  10.,  10.,  10.,  10.]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "Result of FIRST multiplication:\n",
      "tensor([[ 0.8972,  0.8972,  0.8972],\n",
      "        [ 5.4540,  5.4540,  5.4540],\n",
      "        [ 4.4307,  4.4307,  4.4307]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "Result of SECOND multiplication:\n",
      "tensor([[ 15.2529,  15.2529,  15.2529,  15.2529,  15.2529],\n",
      "        [ 92.7180,  92.7180,  92.7180,  92.7180,  92.7180],\n",
      "        [ 75.3217,  75.3217,  75.3217,  75.3217,  75.3217]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(3,2).double()\n",
    "print(data)\n",
    "weight_matrix1 = torch.tensor([[2,2,2],[5,5,5]]).double()\n",
    "print(weight_matrix1)\n",
    "weight_matrix2 = torch.tensor([[2,2,2,2,2],[5,5,5,5,5],[10,10,10,10,10]]).double()\n",
    "print(weight_matrix2)\n",
    "prod1 = data.mm(weight_matrix1)\n",
    "prod2 = prod1.mm(weight_matrix2)\n",
    "print(\"\\n\\nResult of FIRST multiplication:\")\n",
    "print(prod1)\n",
    "print(\"\\n\\nResult of SECOND multiplication:\")\n",
    "print(prod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.ones(2, 3, requires_grad=True)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing an operation on a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.,  3.],\n",
      "        [ 3.,  3.,  3.]])\n",
      "None\n",
      "<AddBackward0 object at 0x7f90bd07bac8>\n"
     ]
    }
   ],
   "source": [
    "y1 = x1 + 2\n",
    "print(y1)\n",
    "print(y1.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y is a result of a function thus it has a __.grad_fn__ attribute which specifies the which function it is result of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  27.,  27.],\n",
      "        [ 27.,  27.,  27.]])\n",
      "<MulBackward0 object at 0x7f90bd07b518>\n",
      "tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "z1 = y1 * y1 * 3\n",
    "out = z1.mean()\n",
    "\n",
    "print(z1)\n",
    "print(z1.grad_fn)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geting gradients of __out__ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.,  3.],\n",
      "        [ 3.,  3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward(retain_graph = False)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1275,  0.2828, -0.1401])\n",
      "tensor([-1154.5289,   289.5901,  -143.4415])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.randn(3,requires_grad=True)\n",
    "print(x2)\n",
    "\n",
    "y2 = x2 * 2\n",
    "while y2.data.norm() < 1000:\n",
    "    y2 = y2 * 2\n",
    "\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  102.4000,  1024.0000,     0.1024])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y2.backward(gradients)\n",
    "\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example from CS231n__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2., dtype=torch.float64)\n",
      "tensor(6., dtype=torch.float64)\n",
      "tensor(4., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(-1., dtype=torch.float64)\n",
      "tensor(0.3679, dtype=torch.float64)\n",
      "tensor(1.3679, dtype=torch.float64)\n",
      "tensor(0.7311, dtype=torch.float64)\n",
      "Gradients:\n",
      "tensor([-0.1966, -0.0197], dtype=torch.float64)\n",
      "tensor([ 0.3932,  0.0393], dtype=torch.float64)\n",
      "tensor([-0.3932, -0.0393], dtype=torch.float64)\n",
      "tensor([-0.5898, -0.0590], dtype=torch.float64)\n",
      "tensor([ 0.1966,  0.0197], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(2,requires_grad=True,dtype=torch.float64)\n",
    "b1 = torch.tensor(-1,requires_grad=True,dtype=torch.float64)\n",
    "a2 = torch.tensor(-3,requires_grad=True,dtype=torch.float64)\n",
    "b2 = torch.tensor(-2,requires_grad=True,dtype=torch.float64)\n",
    "c1 = torch.tensor(-3,requires_grad=True,dtype=torch.float64)\n",
    "\n",
    "prod1 = a1 * b1\n",
    "prod2 = a2 * b2\n",
    "print(prod1)\n",
    "print(prod2)\n",
    "\n",
    "sum1 = prod1 + prod2\n",
    "print(sum1)\n",
    "\n",
    "sum2 = sum1 + c1\n",
    "\n",
    "print(sum2)\n",
    "sum2 = sum2 * (-1)\n",
    "print(sum2)\n",
    "sum2 = torch.exp(sum2.double())\n",
    "print(sum2)\n",
    "sum2 = sum2 + 1\n",
    "print(sum2)\n",
    "sum2 = 1/sum2\n",
    "print(sum2)\n",
    "print(\"Gradients:\")\n",
    "grads = torch.tensor([1,0.1],dtype=torch.double)\n",
    "sum2.backward(grads)\n",
    "\n",
    "\n",
    "print(a1.grad)\n",
    "print(b1.grad)\n",
    "print(a2.grad)\n",
    "print(b2.grad)\n",
    "print(c1.grad)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Learnable parameters of the network :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "for i in params:\n",
    "    print(i.size())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inputing a random 32x32 tensor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0481, -0.0928, -0.0783, -0.0048, -0.0606,  0.1002,  0.0634,\n",
      "         -0.0067,  0.0828, -0.1136]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Computing the loss function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.6173)\n",
      "<MseLossBackward object at 0x7f7d87dd4ac8>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target = torch.arange(1, 11)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print(loss)\n",
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f7d87dd4d68>\n",
      "<AddmmBackward object at 0x7f7dda69de80>\n",
      "<ExpandBackward object at 0x7f7d87dd4d68>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BAckpropagation of the error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0198,  0.0075,  0.0763,  0.1378, -0.0207,  0.1505])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 classifier example\n",
    "\n",
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SHOWING SOME IMAGES__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse horse   dog  deer\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d87db68d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DEFINE THE NETWORK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DEFINE LOSS AND OPTIMIZER__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TRAIN THE NETWORK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.205\n",
      "[1,  4000] loss: 1.903\n",
      "[1,  6000] loss: 1.735\n",
      "[1,  8000] loss: 1.603\n",
      "[1, 10000] loss: 1.519\n",
      "[1, 12000] loss: 1.477\n",
      "[2,  2000] loss: 1.433\n",
      "[2,  4000] loss: 1.373\n",
      "[2,  6000] loss: 1.385\n",
      "[2,  8000] loss: 1.350\n",
      "[2, 10000] loss: 1.321\n",
      "[2, 12000] loss: 1.277\n",
      "[3,  2000] loss: 1.229\n",
      "[3,  4000] loss: 1.222\n",
      "[3,  6000] loss: 1.192\n",
      "[3,  8000] loss: 1.201\n",
      "[3, 10000] loss: 1.172\n",
      "[3, 12000] loss: 1.172\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset 3 times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TESTING AGAINST TRAINING DATA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 66 %\n",
      "Accuracy of   car : 79 %\n",
      "Accuracy of  bird : 60 %\n",
      "Accuracy of   cat : 45 %\n",
      "Accuracy of  deer : 40 %\n",
      "Accuracy of   dog : 27 %\n",
      "Accuracy of  frog : 68 %\n",
      "Accuracy of horse : 63 %\n",
      "Accuracy of  ship : 68 %\n",
      "Accuracy of truck : 63 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
